{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the PDF - vuln\n",
    "#loader = PyPDFLoader(\"/Users/stevenslater/Desktop/FinalProject/VulnGuidelines.pdf\")\n",
    "\n",
    "#loading my CV\n",
    "loader = PyPDFLoader(\"/Users/stevenslater/Desktop/CV.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ity classification, using Chain-of Thought Prompting and a newly proposed Prompt Optimisation framework. University of British Colombia – Exchange Year       Vancouver    2022-2023 • Relevant Courses: Artificial Intelligence, Functional Programming, Applied Machine Learning. • Received 2nd Highest Mark in the year for Artificial Intelligence. Hutchesons Grammar School            Glasgow        2007-2019 • Advanced Higher: Computing Science (A), Business Management (A), Mathematics (A)  • Higher:\n"
     ]
    }
   ],
   "source": [
    "print(page.page_content[500:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Text**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using recusrive splitter (played about with chunks will come back to this)\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = r_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graduation 2024 – Projected 1st • Relevant courses: Algorithms and Data Structures, Data Science, Relational Databases, Object-Oriented Programming. • Final Year project: Leveraging Large Language Models for vulnerability classification, using Chain-of Thought Prompting and a newly proposed\n",
      "a newly proposed Prompt Optimisation framework. University of British Colombia – Exchange Year       Vancouver    2022-2023 • Relevant Courses: Artificial Intelligence, Functional Programming, Applied Machine Learning. • Received 2nd Highest Mark in the year for Artificial Intelligence. Hutchesons\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8247618940780516"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "test1 = splits[1].page_content\n",
    "test2 = splits[2].page_content\n",
    "print(test1)\n",
    "print(test2)\n",
    "embedding1 = embedding.embed_query(test1)\n",
    "embedding2 = embedding.embed_query(test2)\n",
    "\n",
    "#Checking similairities between two splits fom the PDF, will use this in retrieval later.\n",
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrevial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriaval\n",
    "#MMR - MAXIMAL MARGINAL RELEVANCE\n",
    "# may not always want  most similar documments, also if have duplicated data can help filter this out\n",
    "#Algorithm:\n",
    "#Query Vector store\n",
    "#Choose a fetch_k most similar responses\n",
    "#Within this choose the k most diverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"– Technology Summer Intern     Glasgow        Summer 2023 • Developed and deployed a full-stack web application using Python’s Flask framework and JavaScript, leveraging cloud computing to stream real-time diagnostic data from Siemens' Internet of Things (IoT) rail network to all employees on any\", metadata={'page': 0, 'source': '/Users/stevenslater/Desktop/CV.pdf'}),\n",
       " Document(page_content='Hutchesons Grammar School            Glasgow        2007-2019 • Advanced Higher: Computing Science (A), Business Management (A), Mathematics (A)  • Higher: Computing Science (A), Mathematics (A), Physics (A), Business Management (A) English (B)  PROFESSIONAL EXPERIENCE          Siemens – Technology', metadata={'page': 0, 'source': '/Users/stevenslater/Desktop/CV.pdf'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simialrity search\n",
    "vectordb.similarity_search(question,k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MMR\n",
    "mmr=vectordb.max_marginal_relevance_search(question,fetch_k=3, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self Query (LLM aided retrieval)\n",
    "#Uses the LLM to split the question into a filter and a search term\n",
    "#good when you want the filter to be on METADATA - Use when making documents in pipeline bigger or using database for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compression Techniques - come back when making pipeline for larger documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"gpt-3.5-turbo\"\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using RetrievalQA chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "#TO build own retreiver build using CustomedRetriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "newquestion=\"what company did I work for in 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": newquestion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 2023, you worked for Siemens, where you developed and deployed a full-stack web application using Python's Flask framework and JavaScript to stream real-time diagnostic data from Siemens' Internet of Things (IoT) rail network to all employees.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new QA Chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What did Steven do at Siemens?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Siemens, Steven developed and deployed a full-stack web application using Python's Flask framework and JavaScript. He leveraged cloud computing to stream real-time diagnostic data from Siemens' Internet of Things (IoT) rail network to all employees.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Did I use Amazon Web services when I was there?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, the individual mentioned that they developed and deployed a full-stack web application at Siemens, leveraging cloud computing to stream real-time diagnostic data from Siemens' Internet of Things (IoT) rail network to all employees on any device. The app was powered by Amazon Web Services, utilizing AWS Relational Database Service and EC2 Instances for the backend database and web server.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
